---
title: "Homework 03"
subtitle: "Logistic Regression"
author: "Sky Liu"
date: "October 2, 2018"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE)
#install.packages("pacman",repos="https://cloud.r-project.org")
pacman::p_load("ggplot2","knitr","arm","foreign","car","Cairo","data.table")
library('dplyr')
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```

# Data analysis 

### 1992 presidential election

The folder `nes` contains the survey data of presidential preference and income for the 1992 election analyzed in Section 5.1, along with other variables including sex, ethnicity, education, party identification, and political ideology.

```{r, echo=FALSE}
nes5200<-read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/nes/nes5200_processed_voters_realideo.dta")
#saveRDS(nes5200,"nes5200.rds")
#nes5200<-readRDS("nes5200.rds")

nes5200_dt <- data.table(nes5200)
yr <- 1992
nes5200_dt_s<-nes5200_dt[ year==yr & presvote %in% c("1. democrat","2. republican")& !is.na(income)]
nes5200_dt_s<-nes5200_dt_s[,vote_rep:=1*(presvote=="2. republican")]
nes5200_dt_s$income <- droplevels(nes5200_dt_s$income)
```

1.  Fit a logistic regression predicting support for Bush given all these inputs. Consider how to include these as regression predictors and also consider possible interactions.

```{r}
Nes <-  nes5200_dt_s %>%
       select(vote_rep,age,income,gender,race,educ1,partyid7,ideo,rlikes)
Nes <- na.omit(Nes)#clean rows with NAs
Nes$age <- Nes$age - mean(Nes$age)#center the age
```

2. Evaluate and compare the different models you have fit. Consider coefficient estimates and standard errors, residual plots, and deviances.

```{r}
#Nes_glm_1 contains all variables
Nes_glm_1 <- glm(vote_rep ~ ., family=binomial(), data = Nes )
summary(Nes_glm_1)
#Nes_glm_2 excludes educ and income variable since they are not very significant
Nes_glm_2 <- glm(vote_rep ~ age + gender  + race  + partyid7 + ideo + rlikes , family=binomial(),data = Nes)
summary(Nes_glm_2)

par(mfrow=c(1,2))
coefplot(Nes_glm_1)
coefplot(Nes_glm_2)
par(mfrow=c(1,2))
binnedplot(fitted(Nes_glm_1),resid(Nes_glm_1,type="response"))
binnedplot(fitted(Nes_glm_2),resid(Nes_glm_2,type="response"))

```

The AIC value of model1 is 500.64 and the residual deviance is 454.64.
The AIC value of model2 is 492.38 and the residual deviance is 460.38
Although the first model has smaller residual deviance, that is becasue model1 simply has more predictors. The AIC value of modle 2 is actually lower and from the residual plots and coefficient plots we can see that in model 2, more coefficients are significant and more residuals falls inside 95% error bounds.
Therefore, the second model is a better fit. 

3. For your chosen model, discuss and compare the importance of each input variable in the prediction.

```{r}
kable(summary(Nes_glm_2)$coef,digits=3)
```
From the coefficient summary and plot we can see that race being black, party identification being independent or republic, ideology being conservative have strong influences on voting for Bush Gender is less influential.

The intercept infers that the possibility to vote for Bush for a strong demograt liberal white male at the average age with republic president candidate affect level at 0 is $logit^{-1}(-3.519) = 0.029$.

Example for coefficient interpretation:

The coefficient of race2. black being -2.175 infers that if the voter is a black male other than a white male, corresponds to a negative difference in the probability of voting for Bush is about  54%  ($\frac{-2.175}{4} = -0.54$)

### Graphing logistic regressions: 

the well-switching data described in Section 5.4 of the Gelman and Hill are in the folder `arsenic`.  

```{r, echo=FALSE}
wells <- read.table("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat", header=TRUE)
wells_dt <- data.table(wells)
```

1. Fit a logistic regression for the probability of switching using log (distance to nearest safe well) as a predictor.
```{r}
str(wells_dt)
Wells_glm_1 <- glm(switch ~ log(dist), family=binomial(link = "logit"),data = wells_dt)
Wells_glm_1
```

2. Make a graph similar to Figure 5.9 of the Gelman and Hill displaying Pr(switch) as a function of distance to nearest safe well, along with the data.
```{r}
jitter.binary <- function(a, jit=.05){
  ifelse(a==0, runif(length(a), 0, jit), runif(length(a), 1-jit,1))
}
switch.jitter <- jitter.binary(wells_dt$switch)
plot(log(wells_dt$dist), switch.jitter)
curve(invlogit(coef(Wells_glm_1)[1]+coef(Wells_glm_1)[2]*x), add = TRUE)
```

3. Make a residual plot and binned residual plot as in Figure 5.13.
```{r}
par(mfrow=c(1,2))
plot(fitted(Wells_glm_1),resid(Wells_glm_1)); abline(h=0,lty=2)
binnedplot(fitted(Wells_glm_1),resid(Wells_glm_1,type="response"))
```

4. Compute the error rate of the fitted model and compare to the error rate of the null model.

```{r}
predicted_1 <- fitted(Wells_glm_1)
error_rate_1 <- mean ((predicted_1>0.5 & wells_dt$switch==0) | (predicted_1<.5 & wells_dt$switch==1))
error_rate_1

#null model
Wells_glm_null <- glm(switch ~ 1, family=binomial(link = "logit"),data = wells_dt)
predicted_null <- fitted(Wells_glm_null)
error_rate_null <- mean ((predicted_null>0.5 & wells_dt$switch==0) | (predicted_null<.5 & wells_dt$switch==1))
error_rate_null
```
The error rate of the null model is higher.


5. Create indicator variables corresponding to `dist < 100`, `100 =< dist < 200`, and `dist > 200`. Fit a logistic regression for Pr(switch) using these indicators. With this new model, repeat the computations and graphs for part (1) of this exercise.

```{r}
wells_dt$distlevel[wells_dt$dist<100] <- 1
wells_dt$distlevel[wells_dt$dist>200] <- 3
wells_dt$distlevel[wells_dt$dist>=100  & wells_dt$dist<=200] <- 2

Wells_glm_2 <- glm(switch ~ distlevel, family=binomial(link = "logit"),data = wells_dt)
summary(Wells_glm_2)

jitter.binary <- function(a, jit=.05){
  ifelse(a==0, runif(length(a), 0, jit), runif(length(a), 1-jit,1))
}
switch.jitter <- jitter.binary(wells_dt$switch)
plot(log(wells_dt$distlevel), switch.jitter)
curve(invlogit(coef(Wells_glm_2)[1]+coef(Wells_glm_2)[2]*x), add = TRUE)

par(mfrow=c(1,2))
plot(fitted(Wells_glm_2),resid(Wells_glm_2)); abline(h=0,lty=2)
binnedplot(fitted(Wells_glm_2),resid(Wells_glm_2,type="response"))

predicted_2 <- fitted(Wells_glm_2)
error_rate_2 <- mean ((predicted_2>0.5 & wells_dt$switch==0) | (predicted_2<.5 & wells_dt$switch==1))
error_rate_2
```

### Model building and comparison: 
continue with the well-switching data described in the previous exercise.

1. Fit a logistic regression for the probability of switching using, as predictors, distance, `log(arsenic)`, and their interaction. Interpret the estimated coefficients and their standard errors.

```{r}
Arsen_glm_1 <- glm(switch ~ dist * log(arsenic), family=binomial(link="logit"), data = wells_dt)
summary(Arsen_glm_1)
```

Constant term: When the distance to the nearest safe well and the arsenic level of the current well are 0,  the estimated probability of switching is $logit^{-1}(0.49) = 0.62$.This constant term is not interpretable becuase arsenic levels always exceed 0.5. Instead, we can evaluate the prediction at the average values of $dist = 48$ and $arsenic = 1.66$, where the probability of switching is $logit^{-1}(0.49 -0.008* 48 + 0.98  log(1.66) - 0.002 * 48  log(1.66) ) = 0.635$

Coefficient for distance: When the arsenic level of the current well are 0, this corresponds to comparing two wells that differ by 1 in distance. This is still not interpretable. Thus, we evaluate the average value arsenic = 1.66, where distance has a coefficient of$-0.0087 -0.002 * log(1.66) = —0.0097$ on the logit scale. To quickly interpret this on the probability scale, we divide the coefficients by 4: —0.0097/4 = -0.0024. Thus, at the average level of arsenic (on the logit scale) in the data, each increading in meter of distance corresponds to an approximate 0.2% negative difference in probability of switching.

Coefficient for arsenic: When the distance is 0, this corresponds to comparing two wells that differ by 0.98 in distance. This is still not interpretable. Thus, we evaluate the predictive difference with respect to distance by computing the derivative at the average value of distance = 48, where arsenic level (on the logit scale)  has a coefficient o $0.98 — 0.002 *48 = 0.884$ on the logit scale.  To quickly interpret this on the probability scale, we divide the coefficients by 4: 0.884/4 = 0.221. Thus, at the average level of distance in the data, each increading in unit of arsenic level (on the logit scale) corresponds to an approximate 0.221% positive difference in probability of switching.

Interaction of arsenic (on the logit scale) and distance: a difference of arsenic (on the logit scale) corresponds to a difference of -0.002 in the coefficient for distance. As we have already seen, arsenic (on the logit scale) has a positive coefficient on average while distance has a negative coefficient on average; thus increasing distance decreases arsenic’s positive association. This makes sense: people walking further distance could be less awear of  the risks of arsenic and thus less sensitive to increasing arsenic levels (or, conversely, less in a hurry to switch from wells with arsenic levels that are relatively low).

2. Make graphs as in Figure 5.12 to show the relation between probability of switching, distance, and arsenic level.

```{r}
par(mfrow=c(1,2))
plot(wells_dt$dist, switch.jitter, xlim = c(0, max(wells_dt$dist)));curve(invlogit(cbind(1, x, .5, .5*x) %*% coef(Arsen_glm_1)), add= TRUE);curve(invlogit(cbind(1, x, 1, 1*x) %*% coef(Arsen_glm_1)), add= TRUE)
plot(log(wells_dt$arsenic), switch.jitter, xlim = c(0, max(log(wells_dt$arsenic))));curve(invlogit(cbind(1, 0, x, 0*x) %*% coef(Arsen_glm_1)), add= TRUE);curve(invlogit(cbind(1, 50, x, 50*x) %*% coef(Arsen_glm_1)), add= TRUE)
```

3. Following the procedure described in Section 5.7, compute the average predictive differences corresponding to:
i. A comparison of dist = 0 to dist = 100, with arsenic held constant. 
ii. A comparison of dist = 100 to dist = 200, with arsenic held constant.
iii. A comparison of arsenic = 0.5 to arsenic = 1.0, with dist held constant. 
iv. A comparison of arsenic = 1.0 to arsenic = 2.0, with dist held constant.
Discuss these results.

```{r}
b <- coef(Arsen_glm_1)
b
```

$Pr(switch =1) = logit^{-1}(0.49 -.0087distance + 0.9log(arsenic) - 0.0023distance*log(arsenic))$

```{r}
delta <- invlogit (b[1] + b[2]*100 + b[3]*log(wells_dt$arsenic) ) -
invlogit (b[1] + b[2]*0 + b[3]*log(wells_dt$arsenic) )
print (mean(delta))
```
The result is -0.19, implying that, on average in the data, households that are 100 meters from the nearest safe well are 19% less likely to switch, compared to househoulds that are right next to the nearest safe well, at the same arsenic levels.



```{r}
delta <- invlogit (b[1] + b[2]*200 + b[3]*log(wells_dt$arsenic) ) -
invlogit (b[1] + b[2]*100 + b[3]*log(wells_dt$arsenic) )
print (mean(delta))
```
The result is -0.188, implying that, on average in the data, households that are 200 meters from the nearest safe well are 18.8% less likely to switch, compared to househoulds that are 100 meters from the nearest safe well, at the same arsenic levels.


```{r}
delta <- invlogit (b[1] + b[2]*wells_dt$dist + b[3]*1 ) -
invlogit (b[1] + b[2]*wells_dt$dist + b[3]*0.5 )
print (mean(delta))
```
The result is 0.103, implying that, on average in the data, households that are about 1 arsenic level are 10.3% more likely to switch, compared to househoulds that are about 0.5 arsenic level, at the same distance from the nearest safe well.

```{r}
delta <- invlogit (b[1] + b[2]*wells_dt$dist + b[3]*2 ) -
invlogit (b[1] + b[2]*wells_dt$dist + b[3]*1 )
print (mean(delta))
```
The result is 0.143, implying that, on average in the data, households that are about 2 arsenic level are 14.3% more likely to switch, compared to househoulds that are about 1 arsenic level, at the same distance from the nearest safe well.


### Building a logistic regression model: 
the folder rodents contains data on rodents in a sample of New York City apartments.

Please read for the data details.
http://www.stat.columbia.edu/~gelman/arm/examples/rodents/rodents.doc

```{r read_rodent_data, echo=FALSE}
apt.subset.data <- read.table ("http://www.stat.columbia.edu/~gelman/arm/examples/rodents/apt.subset.dat", header=TRUE)
apt_dt <- data.table(apt.subset.data)
setnames(apt_dt, colnames(apt_dt),c("y","defects","poor","race","floor","dist","bldg")
)
invisible(apt_dt[,asian := race==5 | race==6 | race==7])
invisible(apt_dt[,black := race==2])
invisible(apt_dt[,hisp  := race==3 | race==4])

```

1. Build a logistic regression model to predict the presence of rodents (the variable y in the dataset) given indicators for the ethnic groups (race). Combine categories as appropriate. Discuss the estimated coefficients in the model.

```{r}
apt_dt$racecate <- 'Other'
apt_dt$racecate[apt_dt$asian] <- 'Asian'
apt_dt$racecate[apt_dt$black] <- 'Black'
apt_dt$racecate[apt_dt$hisp] <- 'Hisp'

Race_glm_1 <- glm(y~ racecate,family=binomial(link="logit"), data = apt_dt )
summary(Race_glm_1)
```
At the average level of other inputs(defects, poor, dist) in the data, differnet race between asian and black corresponds to an approximate 24% positive difference in probability of the presence of rodents ($\frac{0.98}{4} = 0.24$). The differnet race between asian and hisp corresponds to an approximate 28.8% positive difference in probability of the presence of rodents ($\frac{1.15}{4} = 0.288$). The differnet race between asian and other corresponds to an approximate 13.8% negative difference in probability of the presence of rodents ($\frac{-0.55}{4} = -0.138$). 


2. Add to your model some other potentially relevant predictors describing the apartment, building, and community district. Build your model using the general principles explained in Section 4.6 of the Gelman and Hill. Discuss the coefficients for the ethnicity indicators in your model.

```{r}

Apt_glm <- glm(y ~defects+poor+dist+racecate,family=binomial(link="logit"),data = apt_dt)
summary(Apt_glm)
```
At the average level of other inputs(defects, poor, dist) in the data, differnet race between asian and black corresponds to an approximate 16% positive difference in probability of the presence of rodents ($\frac{0.64}{4} = 0.16$). The differnet race between asian and hisp corresponds to an approximate 19.5% positive difference in probability of the presence of rodents ($\frac{0.78}{4} = 0.195$). The differnet race between asian and other corresponds to an approximate 11% negative difference in probability of the presence of rodents ($\frac{-0.43}{4} = -0.11$). 


# Conceptual exercises.

### Shape of the inverse logit curve

Without using a computer, sketch the following logistic regression lines:

1. $Pr(y = 1) = logit^{-1}(x)$
2. $Pr(y = 1) = logit^{-1}(2 + x)$
3. $Pr(y = 1) = logit^{-1}(2x)$
4. $Pr(y = 1) = logit^{-1}(2 + 2x)$
5. $Pr(y = 1) = logit^{-1}(-2x)$


![Sketch](sketch.png){.callout}
### 
In a class of 50 students, a logistic regression is performed of course grade (pass or fail) on midterm exam score (continuous values with mean 60 and standard deviation 15). The fitted model is $Pr(pass) = logit^{-1}(-24+0.4x)$.

1. Graph the fitted model. Also on this graph put a scatterplot of hypothetical data consistent with the information given.

```{r}
score <- rnorm(50, mean=60, sd = 15)
Pr_pass <- invlogit(-24 + 0.4*score)
pass <- ifelse(Pr_pass>.5,1,0)
ggplot(data.frame(score, pass), aes(x=score, y = pass)) +
  geom_point() +
  stat_function(fun=function(x) invlogit(-24 + 0.4 * x)) +
  labs(x="Midterm Exam Score", y="Pass or Fail") 
```

2. Suppose the midterm scores were transformed to have a mean of 0 and standard deviation of 1. What would be the equation of the logistic regression using these transformed scores as a predictor?

The midterm scores were transformed to have a mean of 0 and standard deviation of 1, means  $trans_score = \frac{score-mean}{sd} = \frac{score-60}{15}$ , therefore, $Pr(pass) = logit^{-1}(6x)$.



3. Create a new predictor that is pure noise (for example, in R you can create `newpred <- rnorm (n,0,1)`). Add it to your model. How much does the deviance decrease?

```{r}
newpred <- rnorm(50,0,1)

deviance(glm(Pr_pass ~ score , family = "binomial"))-deviance(glm(Pr_pass ~ score + newpred, family = "binomial"))
```

### Logistic regression

You are interested in how well the combined earnings of the parents in a child's family predicts high school graduation. You are told that the probability a child graduates from high school is 27% for children whose parents earn no income and is 88% for children whose parents earn $60,000. Determine the logistic regression model that is consistent with this information. (For simplicity you may want to assume that income is measured in units of $10,000).


$Pr(graduation from high school)  = logit^{-1}( -0.9946 + 0.4978 * parents_earning) $

### Latent-data formulation of the logistic model: 
take the model $Pr(y = 1) = logit^{-1}(1 + 2x_1 + 3x_2)$ and consider a person for whom $x_1 = 1$ and $x_2 = 0.5$. Sketch the distribution of the latent data for this person. Figure out the probability that $y=1$ for the person and shade the corresponding area on your graph.

```{r}
epsilon<-rlogis(500,0,1)
z_latent<-1+2*1+3*0.5+epsilon
density<-dlogis(z_latent)
data<-data.frame(cbind(epsilon,z_latent,density))
ggplot(data,mapping=(aes(x=z_latent,y=density)))+geom_line()+geom_area(mapping=aes(x=ifelse(z_latent>=0,z_latent,0)),fill="gray")+ylim(0,0.3)

```

### Limitations of logistic regression: 

consider a dataset with $n = 20$ points, a single predictor x that takes on the values $1, \dots , 20$, and binary data $y$. Construct data values $y_{1}, \dots, y_{20}$ that are inconsistent with any logistic regression on $x$. Fit a logistic regression to these data, plot the data and fitted curve, and explain why you can say that the model does not fit the data.

```{r}

x<-c(1:20)
y<-rep(0,20)
L_glm<-glm(y~x,family = binomial(link = "logit"))
ggplot(L_glm, aes(x=x, y = y)) +
  geom_point() +
  stat_function(fun=function(x) invlogit(coef(L_glm)[1] + coef(L_glm)[2] * x)) +
  labs(x="x", y="y") 
```
From the plot we can see that the line does not fit with the dots.

### Identifiability: 

the folder nes has data from the National Election Studies that were used in Section 5.1 of the Gelman and Hill to model vote preferences given income. When we try to fit a similar model using ethnicity as a predictor, we run into a problem. Here are fits from 1960, 1964, 1968, and 1972:

```{r, echo=FALSE}
nes5200_dt_d<-nes5200_dt[ presvote %in% c("1. democrat","2. republican")& !is.na(income)]
nes5200_dt_d<-nes5200_dt_d[,vote_rep:=1*(presvote=="2. republican")]
nes5200_dt_d$income <- droplevels(nes5200_dt_d$income)

nes5200_dt_d$income <- as.integer(nes5200_dt_d$income)
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1960)))
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1964)))
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1968)))
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1972)))

```

What happened with the coefficient of black in 1964? Take a look at the data and figure out where this extreme estimate came from. What can be done to fit the model in 1964?


No black people voted for Republican. Take out black people in data to make analysis in subcategoeis.


# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opinions.

